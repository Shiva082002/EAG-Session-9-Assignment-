[
  {
    "doc": "ch1n.pdf",
    "chunk": "## Chapter 1 # Data Mining In this intoductory chapter we begin with the essence of data mining and a discussion of how data mining is treated by the various disciplines that contribute to this field. We cover \u201cBonferroni\u2019s Principle,\u201d which is really a warning about overusing the ability to mine data. This chapter is also the place where we summarize a few useful ideas that are not data mining per se, but are useful in understanding some important data-mining concepts. These include the TF.IDF measure of word importance, behavior of hash functions and indexes, and identities involving e, the base of natural logarithms. Finally, we give an outline of the topics covered in the balance of the book. ### 1.1 What is Data Mining? In the 1990\u2019s \u201cdata mining\u201d was an exciting and popular new concept. Around 2010, people instead started to speak of \u201cbig data.\u201d Today, the popular term is \u201cdata science.\u201d However, during all this time, the concept remained the same: use the most powerful hardware, the most powerful programming systems, and the most efficient algorithms to solve problems in science, commerce, healthcare, government, the humanities, and many other fields of human endeavor. 1.1.1 Modeling To many, data mining is the process of creating a model from data, often by the process of machine learning, which we mention in Section 1.1.3 and discuss more fully in Chapter 12. However, more generally, the objective of data mining is an algorithm. For instance, we discuss locality-sensitive hashing in Chapter 3 and a number of stream-mining algorithms in Chapter 4, none of which involve a model. Yet in many important applications, the hard part is creating the model, and once the model is available, the algorithm to use the model is straightforward. 1 2 CHAPTER 1. DATA MINING Example 1.1 : Consider the problem of detecting emails that are phishing attacks. The most common approach is to build a model of phishing emails, perhaps by examining emails that people have recently reported as phishing attacks and looking for the words or phrases that appear unusually often in those emails, such as \u201cNigerian prince\u201d or \u201cverify account.\u201d The model could be weights on words, with positive weights for words that appear frequently in phishing emails and negative weights for words that do not. Then the algorithm to detect phishing emails is simple. Apply the model to each email, that is, sum the weights of the words in that email, and say the email is phishing if and only if the sum is positive. Finding the best weights is a difficult problem, one we shall take up in Section 12.2. \u2737 1.1.2 Statistical Modeling Statisticians were the first to use the term \u201cdata mining.\u201d Originally, \u201cdata mining\u201d or \u201cdata dredging\u201d was a derogatory term referring to attempts to extract information that was not supported by the data. Section 1.2 illustrates the sort of errors one can make by trying to extract what really isn\u2019t in the data. Today, \u201cdata mining\u201d has taken on a positive meaning. Now, statisticians view data mining as the construction",
    "chunk_id": "ch1n_0"
  },
  {
    "doc": "ch1n.pdf",
    "chunk": "of a statistical model, that is, an underlying distribution from which the visible data is drawn. Example 1.2 : Suppose our data is a set of numbers. This data is much simpler than data that would be data-mined, but it will serve as an example. A statistician might decide that the data comes from a Gaussian distribution and use a formula to compute the most likely parameters of this Gaussian. The mean and standard deviation of this Gaussian distribution completely characterize the distribution and would become the model of the data. \u2737 1.1.3 Machine Learning There are some who regard data mining as synonymous with machine learning. There is no question that some data mining appropriately uses algorithms from machine learning. Machine-learning practitioners use the data as a training set, to train an algorithm of one of the many types used for machine-learning, such as Bayes nets, support-vector machines, decision trees, hidden Markov models, and a great variety of others. There are situations where using data in this way makes sense. The typical case where machine learning is a good approach is when we have little idea of what the data says about the problem we are trying to solve. For example, it is rather unclear what it is about movies that makes certain movie-goers like or dislike it. Thus, in answering the \u201cNetflix challenge\u201d to devise an algorithm that predicts the ratings of movies by users, based on a sample of their responses, machine-learning algorithms have proved quite successful. We shall discuss a simple form of this type of algorithm in Section 9.4. 1.1. WHAT IS DATA MINING? 3 However, machine learning can be uncompetitive in situations where we can describe the goals of the mining more directly. An interesting case in point is the attempt by WhizBang! Labs [1] to use machine learning to locate people\u2019s resumes on the Web. It was not able to do better than algorithms designed by hand to look for some of the obvious words and phrases that appear in the typical resume. Since everyone who has looked at or written a resume has a pretty good idea of what resumes contain, there was no mystery about what makes a Web page be a resume. Thus, there was no advantage to machinelearning over the direct design of an algorithm to discover resumes. Another problem with some machine-learning methods is that they often yield a model that, while it may be quite accurate, is not explainable. In some cases, explainability is not important. For example, if you ask Google why it has classified a gmail as spam, it usually says something like \u201cit looks like other messages that people have identified as spam.\u201d That is, the email matches whatever model of spam Google has developed that day, undoubtedly using a technique from the arsenal of machine-learning algorithms. That explanation is probably satisfactory. We really don\u2019t care what Google does, as long as it makes the correct spam/not-spam decision. On the other hand, consider an automobile-insurance company that creates a model of the risk associated with each",
    "chunk_id": "ch1n_1"
  },
  {
    "doc": "ch1n.pdf",
    "chunk": "driver and assigns different premiums to each, according to the model. If your premium goes up, you might well want an explanation of what the new model is doing and why it changed the estimate of your risk. Unfortunately, in many machine-learning methods, especially \u201cdeep learning,\u201d where the model involves layer upon layer of small elements, each of which makes a decision based on inputs from the previous layer, it may not be possible to give a coherent explanation of what the model is doing. 1.1.4 Computational Approaches to Modeling In contrast to the statisical approach, computer scientists tend to look at data mining as an algorithmic problem. In this case, a model of the data is simply the answer to a complex query about that data. For instance, given the set of numbers of Example 1.2, we might compute their average and standard deviation. Note that these values might not be the parameters of the Gaussian that best fits the data, although they will almost certainly be very close if the size of the data is large, and the source of the data is truly Gaussian. There are many different approaches to modeling data. We have already mentioned the possibility of constructing a random process whereby the data could have been generated. Most other approaches to modeling can be described as either 1. Summarizing the data succinctly and approximately, or 2. Extracting the most prominent features of the data and ignoring the rest. 1 This startup attempted to use machine learning to mine large-scale data, and hired many of the top machine-learning people to do so. Unfortunately, it was not able to survive. 4 CHAPTER 1. DATA MINING We shall explore these two approaches in the following sections. 1.1.5 Summarization One of the most interesting forms of summarization is the PageRank idea, which made Google successful and which we shall cover in Chapter 5. In this form of Web mining, the entire complex structure of the Web is summarized by a single number for each page. This number, the \u201cPageRank\u201d of the page, is (oversimplifying somewhat) the probability that a random walker on the graph would be at that page at any given time. Remarkably, this ranking reflects very well the \u201cimportance\u201d of the page \u2013 the degree to which typical searchers would like that page returned as an answer to their search query. Another important form of summary \u2013 clustering \u2013 will be covered in Chapter 7. Here, data is viewed as points in a multidimensional space. Points that are \u201cclose\u201d in this space are assigned to the same cluster. The clusters themselves are summarized, perhaps by giving the centroid of the cluster and the average distance from the centroid of points in the cluster. These cluster summaries then become the summary of the entire data set. Example 1.3 : A famous instance of clustering to solve a problem took place long ago in London, and it was done entirely without computers. [2] The physician John Snow, dealing with a Cholera outbreak plotted the cases on a map of the",
    "chunk_id": "ch1n_2"
  },
  {
    "doc": "ch1n.pdf",
    "chunk": "city. A small illustration suggesting the process is shown in Fig. 1.1. **Image:** [No caption returned] **Image:** [No caption returned] **Image:** [No caption returned] Figure 1.1: Plotting cholera cases on a map of London The cases clustered around some of the intersections of roads. These inter sections were the locations of wells that had become contaminated; people who 2 See http://en.wikipedia.org/wiki/1854 Broad Street cholera outbreak. 1.2. STATISTICAL LIMITS ON DATA MINING 5 lived nearest these wells got sick, while people who lived nearer to wells that had not been contaminated did not get sick. Without the ability to cluster the data, the cause of Cholera would not have been discovered. \u2737 1.1.6 Feature Extraction The typical feature-based model looks for the most extreme examples of a phenomenon and represents the data by these examples. If you are familiar with Bayes nets, a branch of machine learning and a topic we do not cover in this book, you know how a complex relationship between objects is represented by finding the strongest statistical dependencies among these objects and using only those in representing all statistical connections. Some of the important kinds of feature extraction from large-scale data that we shall study are: 1. Frequent Itemsets. This model makes sense for data that consists of \u201cbaskets\u201d of small sets of items, as in the market-basket problem that we shall discuss in Chapter 6. We look for small sets of items that appear together in many baskets, and these \u201cfrequent itemsets\u201d are the characterization of the data that we seek. The original application of this sort of mining was true market baskets: the sets of items, such as hamburger and ketchup, that people tend to buy together when checking out at the cash register of a store or super market. 2. Similar Items. Often, your data looks like a collection of sets, and the objective is to find pairs of sets that have a relatively large fraction of their elements in common. An example is treating customers at an online store like Amazon as the set of items they have bought. In order for Amazon to recommend something else they might like, Amazon can look for \u201csimilar\u201d customers and recommend something many of these customers have bought. This process is called \u201ccollaborative filtering.\u201d If customers were single-minded, that is, they bought only one kind of thing, then clustering customers might work. However, since customers tend to have interests in many different things, it is more useful to find, for each customer, a small number of other customers who are similar in their tastes, and represent the data by these connections. We discuss similarity in Chapter 3. ### 1.2 Statistical Limits on Data Mining A common sort of data-mining problem involves discovering unusual events hidden within massive amounts of data. This section is a discussion of the problem, including \u201cBonferroni\u2019s Principle,\u201d a warning against overzealous use of data mining. 6 CHAPTER 1. DATA MINING 1.2.1 Total Information Awareness Following the terrorist attack of Sept. 11, 2001, it was noticed that there were four people enrolled in different",
    "chunk_id": "ch1n_3"
  },
  {
    "doc": "ch1n.pdf",
    "chunk": "flight schools, learning how to pilot commercial aircraft, although they were not affiliated with any airline. It was conjectured that the information needed to predict and foil the attack was available in data, but that there was then no way to examine the data and detect suspicious events. The response was a program called TIA, or Total Information Awareness, which was intended to mine all the data it could find, including credit-card receipts, hotel records, travel data, and many other kinds of information in order to track terrorist activity. Now information integration \u2013 the idea of relating and combining different data sources to obtain insights that are not available from any one source \u2013 is often a key step on the way to solving an important problem. TIA naturally caused great concern among privacy advocates, and the project was eventually killed by Congress. It is not the purpose of this book to discuss the difficult issue of the privacy-security tradeoff. However, the prospect of TIA or a system like it does raise many technical questions about its feasibility. In this section, we wish to focus on one particular technical problem: if you look in your data for too many things at the same time, you will see things that look interesting, but are in fact simply statistical artifacts and have no significance. That is, if you search your data for activities that look like terrorist behavior, are you not going to find many innocent activities \u2013 or even illicit activities that are not terrorism \u2013 that will result in visits from the police and maybe worse than just a visit? The answer is that it all depends on how narrowly you define the activities that you look for. Statisticians have seen this problem in many guises and have a theory, which we introduce in the next section, for avoiding this sort of error. 1.2.2 Bonferroni\u2019s Principle Suppose you have a certain amount of data, and you look for events of a certain type within that data. You can expect events of this type to occur, even if the data is completely random, and the number of occurrences of these events will grow as the size of the data grows. These occurrences are \u201cbogus,\u201d in the sense that they have no cause other than that random data will always have some number of unusual features that look significant but aren\u2019t. A theorem of statistics, known as the Bonferroni correction gives a statistically sound way to avoid most of these bogus positive responses to a search through the data. Without going into the statistical details, we offer an informal version, Bonferroni\u2019s principle, that helps us avoid treating random occurrences as if they were real. Calculate the expected number of occurrences of the events you are looking for, on the assumption that data is random. If this number is significantly larger than the number of real instances you hope to find, then you must expect almost anything you find to be bogus, i.e., a statistical artifact rather 1.2. STATISTICAL LIMITS ON DATA MINING 7 than",
    "chunk_id": "ch1n_4"
  },
  {
    "doc": "ch1n.pdf",
    "chunk": "evidence of what you are looking for. This observation is the informal statement of Bonferroni\u2019s principle. In a situation like searching for terrorists, where we expect that there are few terrorists operating at any one time, Bonferroni\u2019s principle says that we may only detect terrorists by looking for events that are so rare that they are unlikely to occur in random data. We give an extended example below. 1.2.3 An Example of Bonferroni\u2019s Principle Suppose there are believed to be some \u201cevil-doers\u201d out there, and we want to detect them. Suppose further that we have reason to believe that periodically, evil-doers gather at a hotel to plot their evil. Let us make the following assumptions about the size of the problem: 1. There are one billion people who might be evil-doers. 2. Everyone goes to a hotel one day in 100. 3. A hotel holds 100 people. Hence, there are 100,000 hotels \u2013 enough to hold the 1% of a billion people who visit a hotel on any given day. 4. We shall examine hotel records for 1000 days. To find evil-doers in this data, we shall look for people who, on two different days, were both at the same hotel. Suppose, however, that there really are no evil-doers. That is, everyone behaves at random, deciding with probability 0.01 to visit a hotel on any given day, and if so, choosing one of the 10 [5] hotels at random. Would we find any pairs of people who appear to be evil-doers? We can do a simple approximate calculation as follows. The probability of any two people both deciding to visit a hotel on any given day is .0001. The chance that they will visit the same hotel is this probability divided by 10 [5], the number of hotels. Thus, the chance that they will visit the same hotel on one given day is 10 [\u2212][9] . The chance that they will visit the same hotel on two different given days is the square of this number, 10 [\u2212][18] . Note that the hotels can be different on the two days. Now, we must consider how many events will indicate evil-doing. An \u201cevent\u201d in this sense is a pair of people and a pair of days, such that the two people were at the same hotel on each of the two days. To simplify the arithmetic, note n that for large n, \ufffd 2 \ufffd is about n [2] /2. We shall use this approximation in what 10 9 follows. Thus, the number of pairs of people is \ufffd 2 \ufffd = 5 \u00d7 10 [17] . The number 1000 of pairs of days is \ufffd 2 \ufffd = 5 \u00d7 10 [5] . The expected number of events that look like evil-doing is the product of the number of pairs of people, the number of pairs of days, and the probability that any one pair of people and pair of days is an instance of the behavior we are looking for. That number is 5 \u00d7 10 [17] \u00d7 5 \u00d7 10",
    "chunk_id": "ch1n_5"
  },
  {
    "doc": "ch1n.pdf",
    "chunk": "[5] \u00d7 10 [\u2212][18] = 250, 000 8 CHAPTER 1. DATA MINING That is, there will be a quarter of a million pairs of people who look like evildoers, even though they are not. Now, suppose there really are 10 pairs of evil-doers out there. The police will need to investigate a quarter of a million other pairs in order to find the real evil-doers. In addition to the intrusion on the lives of half a million innocent people, the work involved is sufficiently great that this approach to finding evil-doers is probably not feasible. 1.2.4 Exercises for Section 1.2 Exercise 1.2.1: Using the information from Section 1.2.3, what would be the number of suspected pairs if the following changes were made to the data (and all other numbers remained as they were in that section)? (a) The number of days of observation was raised to 2000. (b) The number of people observed was raised to 2 billion (and there were therefore 200,000 hotels). (c) We only reported a pair as suspect if they were at the same hotel at the same time on three different days. ! Exercise 1.2.2: Suppose we have information about the supermarket purchases of 100 million people. Each person goes to the supermarket 100 times in a year and buys 10 of the 1000 items that the supermarket sells. We believe that a pair of terrorists will buy exactly the same set of 10 items (perhaps the ingredients for a bomb?) at some time during the year. If we search for pairs of people who have bought the same set of items, would we expect that any such people found were truly terrorists? [3] ### 1.3 Things Useful to Know In this section, we offer brief introductions to subjects that you may or may not have seen in your study of other courses. Each will be useful in the study of data mining. They include: 1. The TF.IDF measure of word importance. 2. Hash functions and their use. 3. Secondary storage (disk) and its effect on running time of algorithms. 4. The base e of natural logarithms and identities involving that constant. 5. Power laws. 3 That is, assume our hypothesis that terrorists will surely buy a set of 10 items in common at some time during the year. We don\u2019t want to address the matter of whether or not terrorists would necessarily do so. 1.3. THINGS USEFUL TO KNOW 9 1.3.1 Importance of Words in Documents In several applications of data mining, we shall be faced with the problem of categorizing documents (sequences of words) by their topic. Typically, topics are identified by finding the special words that characterize documents about that topic. For instance, articles about baseball would tend to have many occurrences of words like \u201cball,\u201d \u201cbat,\u201d \u201cpitch,\u201d \u201crun,\u201d and so on. Once we have classified documents to determine they are about baseball, it is not hard to notice that words such as these appear unusually frequently. However, until we have made the classification, it is not possible to identify these words as characteristic.",
    "chunk_id": "ch1n_6"
  },
  {
    "doc": "ch1n.pdf",
    "chunk": "Thus, classification often starts by looking at documents, and finding the significant words in those documents. Our first guess might be that the words appearing most frequently in a document are the most significant. However, that intuition is exactly opposite of the truth. The most frequent words will most surely be the common words such as \u201cthe\u201d or \u201cand,\u201d which help build ideas but do not carry any significance themselves. In fact, the several hundred most common words in English (called stop words) are often removed from documents before any attempt to classify them. In fact, the indicators of the topic are relatively rare words. However, not all rare words are equally useful as indicators. There are certain words, for example \u201cnotwithstanding\u201d or \u201calbeit,\u201d that appear rarely in a collection of documents, yet do not tell us anything useful. On the other hand, a word like \u201cchukker\u201d is probably equally rare, but tips us off that the document is about the sport of polo. The difference between rare words that tell us something and those that do not has to do with the concentration of the useful words in just a few documents. That is, the presence of a word like \u201calbeit\u201d in a document does not make it terribly more likely that it will appear multiple times. However, if an article mentions \u201cchukker\u201d once, it is likely to tell us what happened in the \u201cfirst chukker,\u201d then the \u201csecond chukker,\u201d and so on. That is, the word is likely to be repeated if it appears at all. The formal measure of how concentrated into relatively few documents are the occurrences of a given word is called TF.IDF (Term Frequency times Inverse Document Frequency). It is normally computed as follows. Suppose we have a collection of N documents. Define f ij to be the frequency (number of occurrences) of term (word) i in document j. Then, define the term frequency TF ij to be: f ij TF ij = max k f kj That is, the term frequency of term i in document j is f ij normalized by dividing it by the maximum number of occurrences of any term (perhaps excluding stop words) in the same document. Thus, the most frequent term in document j gets a TF of 1, and other terms get fractions as their term frequency for this document. The IDF for a term is defined as follows. Suppose term i appears in n i of 10 CHAPTER 1. DATA MINING the N documents in the collection. Then IDF i = log 2 (N/n i ). The TF.IDF score for term i in document j is then defined to be TF ij \u00d7 IDF i . The terms with the highest TF.IDF score are often the terms that best characterize the topic of the document. Example 1.4 : Suppose our repository consists of 2 [20] = 1,048,576 documents. Suppose word w appears in 2 [10] = 1024 of these documents. Then IDF w = log 2 (2 [20] /2 [10] ) = log 2(2 [10] ) = 10.",
    "chunk_id": "ch1n_7"
  },
  {
    "doc": "ch1n.pdf",
    "chunk": "Consider a document j in which w appears 20 times, and that is the maximum number of times in which any word appears (perhaps after eliminating stop words). Then TF wj = 1, and the TF.IDF score for w in document j is 10. Suppose that in document k, word w appears once, while the maximum number of occurrences of any word in this document is 20. Then TF wk = 1/20, and the TF.IDF score for w in document k is 1/2. \u2737 1.3.2 Hash Functions The reader has probably heard of hash tables, and perhaps used them in Java classes or similar packages. The hash functions that make hash tables feasible are also essential components in a number of data-mining algorithms, where the hash table takes an unfamiliar form. We shall review the basics here. First, a hash function h takes a hash-key value as an argument and produces a bucket number as a result. The bucket number is an integer, normally in the range 0 to B \u2212 1, where B is the number of buckets. Hash-keys can be of any type. There is an intuitive property of hash functions that they \u201crandomize\u201d hash-keys. To be precise, if hash-keys are drawn randomly from a reasonable population of possible hash-keys, then h will send approximately equal numbers of hash-keys to each of the B buckets. It would be impossible to do so if, for example, the population of possible hash-keys were smaller than B. Such a population would not be \u201creasonable.\u201d However, there can be more subtle reasons why a hash function fails to achieve an approximately uniform distribution into buckets. Example 1.5 : Suppose hash-keys are positive integers. A common and simple hash function is to pick h(x) = x mod B, that is, the remainder when x is divided by B. That choice works well if our population of hash-keys is all positive integers. 1/Bth of the integers will be assigned to each of the buckets. However, suppose our population is the even integers, and B = 10. Then only buckets 0, 2, 4, 6, and 8 can be the value of h(x), and the hash function is distinctly nonrandom in its behavior. On the other hand, if we picked B = 11, then we would find that 1/11th of the even integers get sent to each of the 11 buckets, so the hash function would work well in this case. \u2737 The generalization of Example 1.5 is that when hash-keys are integers, chosing B so it has any common factor with all (or even most of) the possible hashkeys will result in nonrandom distribution into buckets. Thus, it is normally 1.3. THINGS USEFUL TO KNOW 11 preferred that we choose B to be a prime. That choice reduces the chance of nonrandom behavior, although we still have to consider the possibility that all hash-keys have B as a factor. Of course there are many other types of hash functions not based on modular arithmetic. We shall not try to summarize the options here, but some sources of",
    "chunk_id": "ch1n_8"
  },
  {
    "doc": "ch1n.pdf",
    "chunk": "information will be mentioned in the bibliographic notes. What if hash-keys are not integers? In a sense, all data types have values that are composed of bits, and sequences of bits can always be interpreted as integers. However, there are some simple rules that enable us to convert common types to integers. For example, if hash-keys are strings, convert each character to its ASCII or Unicode equivalent, which can be interpreted as a small integer. Sum the integers before dividing by B. As long as B is smaller than the typical sum of character codes for the population of strings, the distribution into buckets will be relatively uniform. If B is larger, then we can partition the characters of a string into groups of several characters each. Treat the concatenation of the codes for the characters of a group as a single integer. Sum the integers associated with all the groups of a string, and divide by B as before. For instance, if B is around a billion, or 2 [30], then grouping characters four at a time will give us 32-bit integers. The sum of several of these will distribute fairly evenly into a billion buckets. For more complex data types, we can extend the idea used for converting strings to integers, recursively. - For a type that is a record, each of whose components has its own type, recursively convert the value of each component to an integer, using the algorithm appropriate for the type of that component. Sum the integers for the components, and convert the integer sum to buckets by dividing by B. - For a type that is an array, set, or bag of elements of some one type, convert the values of the elements\u2019 type to integers, sum the integers, and divide by B. 1.3.3 Indexes An index is a data structure that makes it efficient to retrieve objects given the value of one or more elements of those objects. The most common situation is one where the objects are records, and the index is on one of the fields of that record. Given a value v for that field, the index lets us retrieve all the records with value v in that field, without having to retrieve all the records in the file. For example, we could have a file of (name, address, phone) triples, and an index on the phone field. Given a phone number, the index allows us to find quickly the record or records with that phone number. There are many ways to implement indexes, and we shall not attempt to survey the matter here. The bibliographic notes give suggestions for further reading. However, a hash table is one simple way to build an index. The field 12 CHAPTER 1. DATA MINING or fields on which the index is based form the hash-key for a hash function. We apply the hash function applied to value of the hash-key for each record, and the record itself is placed in the bucket whose number is determined by the hash function. The bucket could be",
    "chunk_id": "ch1n_9"
  },
  {
    "doc": "ch1n.pdf",
    "chunk": "a list of records in main-memory, or a disk block, for example. Then, given a hash-key value, we can hash it, find the bucket, and need to search only that bucket to find the records with that value for the hash-key. If we choose the number of buckets B to be comparable to the number of records in the file, then there will be relatively few records in any bucket, and we will find few, if any, records in the bucket with a hash key that is not the one we are looking for. Thus, the search for the desired records is quite efficient, compared with searching the entire file for records with the desired hash key. *h* (800\u2212555\u22121212) 0 17 *B* \u22121 |Col1|Sally Jones Maple St 800\u2212555\u22121212<br>Records with h(phone) = 17|Sally Jones|Maple St|800\u2212555\u22121212|Col6|Col7| |---|---|---|---|---|---|---| |.<br>.<br>.|.<br>.<br>.|.<br>.<br>.|.<br>.<br>.|.<br>.<br>.|.<br>.<br>.|.<br>.<br>.| |||||||| |.<br>.<br>.|.<br>.<br>.|.<br>.<br>.|.<br>.<br>.|.<br>.<br>.|.<br>.<br>.|.<br>.<br>.| |||||||| Array of bucket headers Figure 1.2: A hash table used as an index; phone numbers are hashed to buckets, and the entire record is placed in the bucket whose number is the hash value of the phone Example 1.6 : Figure 1.2 suggests what a main-memory index of records with name, address, and phone fields might look like. Here, the index is on the phone field, and buckets are linked lists. We show the phone 800-555-1212 hashed to bucket number 17. There is an array of bucket headers, whose ith element is the head of a linked list for the bucket numbered i. We show, in expanded form, one of the elements of the linked list. It contains a record with name, address, and phone fields. This record is in fact one with the phone number 800-555-1212. Other records in that bucket may or may not have this phone number. We only know that whatever phone number they have is a phone that hashes to 17. \u2737 1.3. THINGS USEFUL TO KNOW 13 1.3.4 Secondary Storage It is important, when dealing with large-scale data, that we have a good understanding of the difference in time taken to perform computations when the data is initially on disk, as opposed to the time needed if the data is initially in main memory. The physical characteristics of disks is another subject on which we could say much, but shall say only a little and leave the interested reader to follow the bibliographic notes. Disks are organized into blocks, which are the minimum units that the operating system uses to move data between main memory and disk. For example, the Windows operating system uses blocks of 64K bytes (i.e., 2 [16] = 65,536 bytes to be exact). It takes approximately ten milliseconds to access (move the disk head to the track of the block and wait for the block to rotate under the head) and read a disk block. That delay is at least five orders of magnitude (a factor of 10 [5] ) slower than the time taken to read a word from main memory, so if all we want to do is access a few bytes, there is an overwhelming benefit to having data in",
    "chunk_id": "ch1n_10"
  },
  {
    "doc": "ch1n.pdf",
    "chunk": "main memory. In fact, if we want to do something simple to every byte of a disk block, e.g., treat the block as a bucket of a hash table and search for a particular value of the hash-key among all the records in that bucket, then the time taken to move the block from disk to main memory will be far larger than the time taken to do the computation. By organizing our data so that related data is on a single cylinder (the collection of blocks reachable at a fixed radius from the center of the disk, and therefore accessible without moving the disk head), we can read all the blocks on the cylinder into main memory in considerably less than 10 milliseconds per block. You can assume that a disk cannot transfer data to main memory at more than a hundred million bytes per second, no matter how that data is organized. That is not a problem when your dataset is a megabyte. But a dataset of a hundred gigabytes or a terabyte presents problems just accessing it, let alone doing anything useful with it. 1.3.5 The Base of Natural Logarithms The constant e = 2.7182818 \u00b7 \u00b7 \u00b7 has a number of useful special properties. In particular, e is the limit of (1 + x [1] [)] [x] [ as][ x][ goes to infinity. The values of this] expression for x = 1, 2, 3, 4 are approximately 2, 2.25, 2.37, 2.44, so you should find it easy to believe that the limit of this series is around 2.72. Some algebra lets us obtain approximations to many seemingly complex expressions. Consider (1 + a) [b], where a is small. We can rewrite the expression as (1+a) [(1][/a][)(][ab][)] . Then substitute a = 1/x and 1/a = x, so we have (1+ [1] [)] [x][(][ab][)] [,] as (1+a) [(1][/a][)(][ab][)] . Then substitute a = 1/x and 1/a = x, so we have (1+ x [1] [)] [x][(][ab][)] [,] which is x [\ufffd] [ab] \ufffd\ufffd1 + x [1] \ufffd Since a is assumed small, x is large, so the subexpression (1 + x [1] [)] [x] [ will be close] to the limiting value of e. We can thus approximate (1 + a) [b], for small a, as e [ab] . x x [\ufffd] [ab] \ufffd Since a is assumed small, x is large, so the subexpression (1 + [1] 14 CHAPTER 1. DATA MINING Similar identities hold when a is negative. That is, the limit as x goes to infinity of (1 \u2212 x [1] [)] [x] [ is 1][/e][. It follows that the approximation (1 +][ a][)] [b] [ =][ e] [ab] holds even when a is a small negative number. Put another way, (1 \u2212 a) [b] is approximately e [\u2212][ab] when a is small. Some other useful approximations follow from the Taylor expansion of e [x] . That is, e [x] = [\ufffd] [\u221e] i=0 [x] [i] [/i][!, or][ e] [x] [ = 1 +][ x][ +][ x] [2] [/][2 +][ x] [3] [/][6 +][ x] [4]",
    "chunk_id": "ch1n_11"
  },
  {
    "doc": "ch1n.pdf",
    "chunk": "[/][24 +][ \u00b7 \u00b7 \u00b7][ . When] x is large, the above series converges slowly, although it does converge because n! grows faster than x [n] for any constant x. However, when x is small, either positive or negative, the series converges rapidly, and only a few terms are necessary to get a good approximation. Example 1.7 : Let x = 1/2. Then [1] 2 [+ 1] 1 48 [+] 384 [+][ \u00b7 \u00b7 \u00b7] e [1][/][2] = 1 + [1] 8 [+ 1] or approximately e [1][/][2] = 1.64844. Let x = \u22121. Then [1] 2 [\u2212] [1] 1 1 1 24 [\u2212] 120 [+] 720 [\u2212] 5040 [+][ \u00b7 \u00b7 \u00b7] e [\u2212][1] = 1 \u2212 1 + [1] [1] 6 [+ 1] 24 or approximately e [\u2212][1] = 0.36786. \u2737 1.3.6 Power Laws There are many phenomena that relate two variables by a power law, that is, a linear relationship between the logarithms of the variables. Figure 1.3 suggests such a relationship. If x is the horizontal axis and y is the vertical axis, then the relationship is log 10 y = 6 \u2212 2 log 10 x. Example 1.8 : We might examine book sales at Amazon.com, and let x represent the rank of books by sales. Then y is the number of sales of the xth best-selling book over some period. The implication of the graph of Fig. 1.3 would be that the best-selling book sold 1,000,000 copies, the 10th best-selling book sold 10,000 copies, the 100th best-selling book sold 100 copies, and so on for all ranks between these numbers and beyond. The implication that above rank 1000 the sales are a fraction of a book is too extreme, and we would in fact expect the line to flatten out for ranks much higher than 1000. Moreover, the slope of the line in Fig. 1.3 is probably much too steep for describing book sales, although a line that drops less precipitously would be close to what happens in practice. \u2737 The general form of a power law relating x and y is log y = b + a log x. If we raise the base of the logarithm (the base doesn\u2019t actually matter in the equation), say e, to the values on both sides of this equation, we get y = e [b] e [a][ log][ x] = e [b] x [a] . Since e [b] is just \u201csome constant,\u201d let us replace it by constant c. Thus, a power law can be written as y = cx [a] for some constants a and c. 1.3. THINGS USEFUL TO KNOW 15 10,000,000 1,000,000 100,000 10,000 1000 100 10 1 **Image:** [No caption returned] 1 10 100 1000 10,000 Figure 1.3: A power law with a slope of \u22122 Example 1.9 : In Fig. 1.3 we see that when x = 1, y = 10 [6], and when x = 1000, y = 1. Making the first substitution, we see 10 [6] = c. The second substitution gives us 1 = c(1000) [a] . Since we now",
    "chunk_id": "ch1n_12"
  },
  {
    "doc": "ch1n.pdf",
    "chunk": "know c = 10 [6], the second equation gives us 1 = 10 [6] (1000) [a], from which we see a = \u22122. That is, the law expressed by Fig. 1.3 is y = 10 [6] x [\u2212][2], or y = 10 [6] /x [2] . \u2737 We shall meet in this book many ways that power laws govern phenomena. Here are some examples: 1. Node Degrees in the Web Graph: Order all pages by the number of inlinks to that page. Let x be the position of a page in this ordering, and let y be the number of in-links to the xth page. Then y as a function of x looks very much like Fig. 1.3. The exponent a is slightly larger than the \u2212 2 shown there; it has been found closer to 2.1. 2. Sales of Products: Order products, say books at Amazon.com, by their sales over the past year. Let y be the number of sales of the xth most popular book. Again, the function y(x) will look something like Fig. 1.3. we shall discuss the consequences of this distribution of sales in Section 9.1.2, where we take up the matter of the \u201clong tail.\u201d 3. Sizes of Web Sites: Count the number of pages at Web sites, and order sites by the number of their pages. Let y be the number of pages at the xth site. Again, the function y(x) follows a power law. 4. Zipf\u2019s Law : This power law originally referred to the frequency of words in a collection of documents. If you order words by frequency, and let y 16 CHAPTER 1. DATA MINING The Matthew Effect Often, the existence of power laws with values of the exponent higher than 1 are explained by the Matthew effect. In the biblical Book of Matthew, there is a verse about \u201cthe rich get richer.\u201d Many phenomena exhibit this behavior, where getting a high value of some property causes that very property to increase. For example, if a Web page has many links in, then people are more likely to find the page and may choose to link to it from one of their pages as well. As another example, if a book is selling well on Amazon, then it is likely to be advertised when customers go to the Amazon site. Some of these people will choose to buy the book as well, thus increasing the sales of this book. be the number of times the xth word in the order appears, then you get a power law, although with a much shallower slope than that of Fig. 1.3. Zipf\u2019s observation was that y = cx [\u2212][1][/][2] . Interestingly, a number of other kinds of data follow this particular power law. For example, if we order states in the US by population and let y be the population of the xth most populous state, then x and y obey Zipf\u2019s law approximately. 1.3.7 Exercises for Section 1.3 Exercise 1.3.1: Suppose there is a repository of ten million documents. What (to the nearest integer) is",
    "chunk_id": "ch1n_13"
  },
  {
    "doc": "ch1n.pdf",
    "chunk": "the IDF for a word that appears in (a) 40 documents (b) 10,000 documents? Exercise 1.3.2: Suppose there is a repository of ten million documents, and word w appears in 320 of them. In a particular document d, the maximum number of occurrences of a word is 15. Approximately what is the TF.IDF score for w if that word appears (a) once (b) five times? ! Exercise 1.3.3: Suppose hash-keys are drawn from the population of all nonnegative integers that are multiples of some constant c, and hash function h(x) is x mod 15. For what values of c will h be a suitable hash function, i.e., a large random choice of hash-keys will be divided roughly equally into buckets? Exercise 1.3.4: In terms of e, give approximations to (a) (1.01) [500] (b) (1.05) [1000] (c) (0.9) [40] Exercise 1.3.5: Use the Taylor expansion of e [x] to compute, to three decimal places: (a) e [1][/][10] (b) e [\u2212][1][/][10] (c) e [2] . 1.4. OUTLINE OF THE BOOK 17 ### 1.4 Outline of the Book This section gives brief summaries of the remaining chapters of the book. Chapter 2 is not about data mining per se. Rather, it introduces us to programming systems that facilitate parallel processing of massive amounts of data. We discuss the cloud-computing architecture, which uses large numbers of connected processors. We discuss in detail programming systems based on MapReduce, and offer MapReduce-based algorithms for a number of common operations using in processing massive datasets. Chapter 3 is about finding similar items. Our starting point is that items can be represented by sets of elements, and similar sets are those that have a large fraction of their elements in common. The key techniques of minhashing and locality-sensitive hashing are explained. These techniques have numerous applications and often give surprisingly efficient solutions to problems that appear impossible for massive data sets. In Chapter 4, we consider data in the form of a stream. The difference between a stream and a database is that the data in a stream is lost if you do not do something about it immediately. Important examples of streams are the streams of search queries at a search engine or clicks at a popular Web site. In this chapter, we see several of the surprising applications of hashing that make management of stream data feasible. Chapter 5 is devoted to a single application: the computation of PageRank. This computation is the idea that made Google stand out from other search engines, and it is still an essential part of how search engines know what pages the user is likely to want to see. Extensions of PageRank are also essential in the fight against spam (euphemistically called \u201csearch engine optimization\u201d), and we shall examine the latest extensions of the idea for the purpose of combating spam. Then, Chapter 6 introduces the market-basket model of data, and its canonical problems of association rules and finding frequent itemsets. In the marketbasket model, data consists of a large collection of baskets, each of which contains a small set of items.",
    "chunk_id": "ch1n_14"
  },
  {
    "doc": "ch1n.pdf",
    "chunk": "We give a sequence of algorithms capable of finding all frequent pairs of items, that is pairs of items that appear together in many baskets. Another sequence of algorithms are useful for finding most of the frequent itemsets larger than pairs, with high efficiency. Chapter 7 examines the problem of clustering. We assume a set of items with a distance measure defining how close or far one item is from another. The goal is to examine a large amount of data and partition it into subsets (clusters), each cluster consisting of items that are all close to one another, yet far from items in the other clusters. Chapter 8 is devoted to on-line advertising and the computational problems it engenders. We introduce the notion of an on-line algorithm \u2013 one where a good response must be given immediately, rather than waiting until we have seen the entire dataset. The idea of competitive ratio is another important concept covered in this chapter; it is the ratio of the guaranteed performance of 18 CHAPTER 1. DATA MINING an on-line algorithm compared with the performance of the optimal algorithm that is allowed to see all the data before making any decisions. These ideas are used to give good algorithms that match bids by advertisers for the right to display their ad in response to a query against the search queries arriving at a search engine. Chapter 9 is devoted to recommendation systems. Many Web applications involve advising users on what they might like. The Netflix challenge is one example, where it is desired to predict what movies a user would like, or Amazon\u2019s problem of pitching a product to a customer based on information about what they might be interested in buying. There are two basic approaches to recommendation. We can characterize items by features, e.g., the stars of a movie, and recommend items with the same features as those the user is known to like. Or, we can look at other users with preferences similar to that of the user in question, and see what they liked (a technique known as collaborative filtering). In Chapter 10, we study social networks and algorithms for their analysis. The canonical example of a social network is the graph of Facebook friends, where the nodes are people, and edges connect two people if they are friends. Directed graphs, such as followers on Twitter, can also be viewed as social networks. A common example of a problem to be addressed is identifying \u201ccommunities,\u201d that is, small sets of nodes with an unusually large number of edges among them. Other questions about social networks are general questions about graphs, such as computing the transitive closure or diameter of a graph, but are made more difficult by the size of typical networks. Chapter 11 looks at dimensionality reduction. We are given a very large matrix, typically sparse. Think of the matrix as representing a relationship between two kinds of entities, e.g., ratings of movies by viewers. Intuitively, there are a small number of concepts, many fewer concepts than there are movies",
    "chunk_id": "ch1n_15"
  },
  {
    "doc": "ch1n.pdf",
    "chunk": "or viewers, that explain why certain viewers like certain movies. We offer several algorithms that simplify matrices by decomposing them into a product of matrices that are much smaller in one of the two dimensions. One matrix relates entities of one kind to the small number of concepts and another relates the concepts to the other kind of entity. If done correctly, the product of the smaller matrices will be very close to the original matrix. Finally, Chapter 12 discusses algorithms for machine learning from very large datasets. Techniques covered include perceptrons, support-vector machines, finding models by gradient descent, nearest-neighbor models, and decision trees. ### 1.5 Summary of Chapter 1 \u2726 Data Mining: This term refers to applying the powerful tools of computer science to solve problems in science, industry, and many other application areas. Frequently, the key to a successful application is building a model 1.6. REFERENCES FOR CHAPTER 1 19 of the data, that is, a summary or relatively succinct representation of the most relevant features of the data. \u2726 Bonferroni\u2019s Principle: If we are willing to view as an interesting feature of data something of which many instances can be expected to exist in random data, then we cannot rely on such features being significant. This observation limits our ability to mine data for features that are not sufficiently rare in practice. \u2726 TF.IDF : The measure called TF.IDF lets us identify words in a collection of documents that are useful for determining the topic of each document. A word has high TF.IDF score in a document if it appears in relatively few documents, but appears in this one, and when it appears in a document it tends to appear many times. \u2726 Hash Functions: A hash function maps hash-keys of some data type to integer bucket numbers. A good hash function distributes the possible hash-key values approximately evenly among buckets. Any data type can be the domain of a hash function. \u2726 Indexes: An index is a data structure that allows us to store and retrieve data records efficiently, given the value in one or more of the fields of the record. Hashing is one way to build an index. \u2726 Storage on Disk : When data must be stored on disk (secondary memory), it takes very much more time to access a desired data item than if the same data were stored in main memory. When data is large, it is important that algorithms strive to keep needed data in main memory. \u2726 Power Laws: Many phenomena obey a law that can be expressed as y = cx [a] for some power a, often around \u22122. Such phenomena include the sales of the xth most popular book, or the number of in-links to the xth most popular page. ### 1.6 References for Chapter 1 [8] is a clear introduction to the basics of data mining. [3] covers data mining principally from the point of view of machine learning and statistics. The difference between the statistical approach and the computational approach to data mining is expressed in [1].",
    "chunk_id": "ch1n_16"
  },
  {
    "doc": "ch1n.pdf",
    "chunk": "For construction of hash functions and hash tables, see [5]. Details of the TF.IDF measure and other matters regarding document processing can be found in [6]. See [4] for more on managing indexes, hash tables, and data on disk. Power laws pertaining to the Web were explored by [2]. The Matthew effect was first observed in [7]. 20 CHAPTER 1. DATA MINING 1. L. Breiman, \u201cStatistical modeling: the two cultures,\u201d Statistical Science 16:3, pp. 199\u2013215, 2001. 2. A. Broder, R. Kumar, F. Maghoul, P. Raghavan, S. Rajagopalan, R. Stata, A. Tomkins, and J. Weiner, \u201cGraph structure in the web,\u201d Computer Networks 33:1\u20136, pp. 309\u2013320, 2000. 3. M.M. Gaber, Scientific Data Mining and Knowledge Discovery \u2014 Principles and Foundations, Springer, New York, 2010. 4. H. Garcia-Molina, J.D. Ullman, and J. Widom, Database Systems: The Complete Book Second Edition, Prentice-Hall, Upper Saddle River, NJ, 2009. 5. D.E. Knuth, The Art of Computer Programming Vol. 3 (Sorting and Searching), Second Edition, Addison-Wesley, Upper Saddle River, NJ, 1998. 6. C.P. Manning, P. Raghavan, and H. Sch\u00a8utze, Introduction to Information Retrieval, Cambridge Univ. Press, 2008. 7. R.K. Merton, \u201cThe Matthew effect in science,\u201d Science 159:3810, pp. 56\u2013 63, Jan. 5, 1968. 8. P.-N. Tan, M. Steinbach, and V. Kumar, Introduction to Data Mining, Addison-Wesley, Upper Saddle River, NJ, 2005.",
    "chunk_id": "ch1n_17"
  },
  {
    "doc": "englishaa.pdf",
    "chunk": "# 04-05-2025 1 1 2 3 4 5 6 7 8 **PARAGRAPH DEVELOPMENT** - A paragraph is an organized set of sentences that deal with a single topic. - Well-organized paragraphs provide the structure in our writing, allowing us to develop the major points we wish to convey to the reader and move smoothly from one point to another. - And they also provide visual structure to the page, indicating subdivisions of our theme or changes of topic. - Long, unbroken blocks of text often appear daunting to the reader. - They look intimidating and can prove difficult to navigate. - Paragraphs help remove some of that \u201cintimidation factor.\u201d - They\u2019re used to break the paper into manageable, accessible \u201cchunks\u201d of information that lead the reader through a logical progression of important points. - Good paragraphing also greatly assists your readers in following a piece of writing. - You can have radical ideas, but if those ideas aren't presented in an organized fashion, you will lose your readers and fail to achieve your goals in writing. **The Basic Rule** - Keep one idea to one paragraph - If you begin to transition into a new idea, it belongs in a new paragraph. - There are some simple ways to tell if you are on the same topic or a new one. - You can have one idea and several bits of supporting evidence within a single paragraph. - You can also have several points in a single paragraph as long as they relate to the overall topic of the paragraph. - If the single points start to get long, then perhaps elaborating on each of them and placing them in their own paragraphs is the route to go. **Elements of a paragraph** - To be as effective as possible, a paragraph should contain unity, coherence, a topic sentence, and adequate development. - As you will see, all of these traits overlap. - Using and adapting them to your individual purposes will help you construct effective paragraphs. **Unity** ----- # 04-05-2025 2 9 10 11 12 13 14 - The entire paragraph should concern itself with a single focus. - If it begins with one focus or major point of discussion, it should not end with another or wander within different ideas. - Example - \"Employees' attitudes at Jonstone Electric Company should be improved. The workers do not feel that they are a working team instead of just individuals. If people felt they were a part of a team, they would not misuse the tools, or deliberately undermine the work of others. Management's attitude toward its employees should also be improved. Managers at Jonstone Electric act as though their employees are incapable of making decisions or doing their own work. Managers treat workers like objects, not human beings.\" - Consider the given example. Note that there are two main ideas presented in this paragraph. - The topic sentence indicates that the paragraph will deal with the subject of \"employees' attitudes,\" but the paragraph shifts unexpectedly to the topic of \"management's attitudes.\" - To",
    "chunk_id": "englishaa_0"
  },
  {
    "doc": "englishaa.pdf",
    "chunk": "achieve unity in this paragraph, the writer should begin a new paragraph when the switch is made from employees to managers. **Coherence** - Coherence is the trait that makes the paragraph easily understandable to a reader. - You can help create coherence in your paragraphs by creating logical bridges and verbal bridges. - Logical bridges - The same idea of a topic is carried over from sentence to sentence. - Successive sentences can be constructed in parallel form. - *e.g. In the first stage, an idea can be generated by using a mind-map and some discussion. In the* *next stage, a clear plan of action can be produced in the form of a list.* - Verbal bridges - Key words can be repeated in several sentences - Synonymous words can be repeated in several sentences - Pronouns can refer to nouns in previous sentences - Transition words can be used to link ideas from different sentences - Example - \"Schools should offer courses to help students with the problems of unemployment. Such a course might begin with a discussion of where to find employment, then cover resume writing ----- 15 16 17 18 19 # 04-05-2025 and interviewing. Algebra and history don't help students with real-world needs. They are required courses that students aren't interested in, and this is frustrating for students who would rather learn about other subjects. If schools offered job-skills courses, students would be well prepared for the difficult task of finding a job once they finish school.\" - Consider the example given above. In this paragraph, the writer begins with the topic of job-skills courses, but veers off onto the topic of algebra and history before returning to the subject of courses on employment. - As a result, the paragraph is disjointed and difficult to understand. **Topic Sentence** - The topic sentence is a sentence that indicates in a general way what idea or thesis the paragraph is going to deal with. - Although not all paragraphs have clear-cut topic sentences, an easy way to make sure your reader understands the topic of the paragraph is to put your topic sentence near the beginning of the paragraph. - Regardless of whether you include an explicit topic sentence or not, you should be able to easily summarize what the paragraph is about. - Examples - \"Many television cartoons contain an unhealthy amount of violence.\" - Notice that this sentence clearly identifies that the key topic of the paragraph is violence in television cartoons. - It also indicates that the remainder of the paragraph will discuss how much violence cartoons typically contain, and how/why this violence is unhealthy for viewers. - \"An increasing number of people in America are enjoying the benefits of organically grown fruits and vegetables.\" - This topic sentence indicates that the remainder of the paragraph will cover the trend in the United States toward eating organic foods. - The reader can also anticipate learning more in this paragraph about the specific benefits of organic foods. **Adequate development** - The topic (which is introduced by the topic",
    "chunk_id": "englishaa_1"
  },
  {
    "doc": "englishaa.pdf",
    "chunk": "sentence) should be discussed fully and adequately. - Again, this varies from paragraph to paragraph, depending on the author's purpose, but writers should be wary of paragraphs that only have two or three sentences. - A good rule of thumb to follow is to make sure that a paragraph contains four to five sentences which explain and elaborate on the topic sentence. # 3 ----- 20 21 22 23 24 25 26 27 - Example - \"The topics of leadership and management are both similar to and different from one another in several important ways. To be effective, a manager should be a good leader. And good leaders know how to manage people effectively.\" - Consider the paragraph given above. - The topic sentence promises to discuss \"several\" points of comparison and contrast between leadership and management, but the remainder of the paragraph falls short of fulfilling this promise. - Only one point of comparison is raised, and this point is left unexplained. - Several questions remain unanswered: How are leaders different from managers? In what specific ways are the two alike? Why must a manager be a good leader to be effective? Why must good leaders know how to manage people effectively? - To achieve adequate development in this paragraph, these questions should be addressed. - Generally speaking, a paragraph should contain between three and five sentences, all of which help clarify and support the main idea of the paragraph. - When a writer begins a new paragraph, it signals to the reader that the writer is changing thoughts or ideas, or is moving on to discuss a different aspect of a main idea. - Here is an example of a paragraph of stacked sentences that lacks logical and verbal bridges: - *My dogs are named Cooper and Calli. Cooper is a Golden retriever and Akita mix. He is a male.* *Calli is a shepherd, Husky and wolf mix. She is a female. Calli was rescued from the pound. Cooper* *was purchased from a breeder. They are close in age. They play together all the time.* - Revised to incorporate bridges and varied sentence structure, the paragraph would read as follows: - *My dogs, Cooper and Calli, are best friends. Cooper, a male retriever and Akita mix, came from a* *breeder. On the other hand, Calli, a shepherd, husky and wolf mix, was rescued from the pound.* *Because they are close in age, they play together all the time.* - Still, the paragraph lacks adequate development. Adequate development is achieved through details, including facts, description, examples, quotes, analysis, explanation, and evaluation. A more developed paragraph would read like this: # 04-05-2025 4 ----- 28 29 30 31 32 33 # 04-05-2025 - *My dogs, Cooper and Calli, are best friends. Cooper, a male golden retriever and Akita mix, came* *from a breeder. On the other hand, Calli, a shepherd, husky and wolf mix, was rescued from the* *pound. Because they are close in age, they play together all the time. For example, the two dogs* *hunt for mice that are attracted",
    "chunk_id": "englishaa_2"
  },
  {
    "doc": "englishaa.pdf",
    "chunk": "by the seed in the chicken coop in the back yard. They also play in* *the kiddie pool I fill with water every morning. Being a golden retriever mix, Cooper should be more* *attracted to the water, but Calli is the one who is always wet from laying in the pool.* - The paragraph, however, has no closure. It just \u201cstops.\u201d A lead in and a final sentence are still needed: - *Cooper keeps Calli active and fit with their constant play. They are truly bonded.* **Methods to develop your paragraph** - Use examples and illustrations - Cite data (facts, statistics, evidence, details, and others) - Examine testimony (what other people say such as quotes and paraphrases) - Use an anecdote or story - Define the terms used in the paragraph - Compare and contrast - Evaluate causes and reasons - Examine effects and consequences - Analyze the topic - Describe the topic - Offer a chronology of an event (time segments) **When to start a new paragraph?** - When you begin a new idea or point. - New ideas should always start in new paragraphs. - If you have an extended idea that spans multiple paragraphs, each new point within that idea should have its own paragraph. - To contrast information or ideas. - Separate paragraphs can serve to contrast sides in a debate, different points in an argument, or any other difference. - When your readers need a pause. Breaks between paragraphs function as a short \"break\" for your readers\u2014adding these in will help your writing be more readable. - You would create a break if the paragraph becomes too long or the material is complex. # 5 ----- # 04-05-2025 6 34 35 36 37 38 - When you are ending your introduction or starting your conclusion. - Your introductory and concluding material should always be in a new paragraph. - Many introductions and conclusions have multiple paragraphs depending on their content, length, and the writer's purpose. **Transitions and signposts** - Two very important elements of paragraphing are signposts and transitions. - Signposts are internal aids to assist readers; they usually consist of several sentences or a paragraph outlining what the article has covered and where the article will be going. - Signposts occur as sentences within an essay indicating to the reader how the essay is structured and how the ideas are arranged. - Transitions are usually one or several sentences that \"transition\" from one idea to the next. - Transitions can be used at the end of most paragraphs to help the paragraphs flow one into the next. - Transitional words form a link to the previous sentence; they include words such as: also, although, likewise, however, subsequently, therefore, rather, also, again, for example. **5-step process to paragraph development** - Step 1. Decide on a controlling idea and create a topic sentence - Step 2. Explain the controlling idea - Step 3. Give an example (or multiple examples) - Step 4. Explain the example(s) - Step 5. Complete the paragraph\u2019s idea or transition into the next paragraph",
    "chunk_id": "englishaa_3"
  },
  {
    "doc": "englishaa.pdf",
    "chunk": "**Classroom Activity** - Prepare a 1.5 page Personal Development Plan for the year 2024 based on SMART Goals - SMART: Specific, Measurable, Attainable, Relevant and Timely - Paragraph 1: Analysis of Personal Current Situation and Development Needs - Paragraph 2: Timeline of activities - Paragraph 3: Summing up -----",
    "chunk_id": "englishaa_4"
  },
  {
    "doc": "Small-Stories-in-English-learnenglishteam.com_.pdf",
    "chunk": "# SMALL STORIES [www.learnenglishteam.com](http://www.learnenglishteam.com) **Image:** [No caption returned] Welcome to our collection of [small stories in English,](https://www.learnenglishteam.com/small-stories-in-english) where you\u2019ll find engaging and meaningful tales that are perfect for readers of all ages. Our stories, inspired by timeless themes and real-life lessons, are crafted to entertain and educate. Whether you're looking for a tale about honesty, love, or adventure, you'll discover stories that are not only enjoyable but also thought-provoking. ----- ## **The Lost Treasure of Pi-Rate Island** **Image:** [No caption returned] On the edge of the vast, sparkling ocean lay two islands known for their legends and mysteries: Pi-Rate Island and A-Rate Island. Pi-Rate Island, famous for its pirate lore, was said to hold a long-lost treasure. A-Rate Island, just a short sail away, was popular among wanderers for its breathtaking views and hidden caves. In the small coastal village of Seaside, young Finn was known for his adventurous spirit and boundless curiosity. His grandfather, a retired sailor, often regaled him with tales of treasure maps and pirate legends. One day, Finn found an old, weathered map tucked away in a dusty corner of his grandfather's attic. The map had faded markings but clearly showed a route to Pi-Rate Island with a large \u201cX\u201d marking the spot where the treasure was buried. With his heart racing, Finn set off on a small sailboat, the \u201cSea Serpent,\u201d to uncover the secrets of Pi-Rate Island. He packed his essentials, including a compass, a rope, and a trusty spyglass, and set sail on a bright, sunny morning. The journey to Pi-Rate Island was filled with adventure, as he navigated through playful dolphins and navigated past swirling whirlpools. Upon arriving at Pi-Rate Island, Finn was greeted by a dense jungle and a maze of rocky cliffs. The island's eerie, fog-covered landscape was both enchanting and foreboding. Finn\u2019s first challenge was to find a way through the tangled jungle. With his map and compass in hand, he ventured deeper into the island. Along the way, Finn encountered a mischievous parrot named Captain Squawk, who had been living on the island for years. Captain Squawk had a colorful plumage and a knack for riddles. \u201cAhoy, young adventurer!\u201d squawked the parrot. \u201cIf ye want to find the treasure, ye must first solve me riddles!\u201d ----- Finn was eager to prove himself. Captain Squawk perched on a branch and recited his first riddle: \u201cI can fly without wings, I can cry without eyes. Wherever I go, darkness flies. What am I?\u201d Finn thought hard, then answered, \u201cA cloud!\u201d The parrot fluttered in approval and revealed a hidden path. As Finn followed the path, he came across an ancient, crumbling pirate fort. The fort was filled with remnants of old battles and dusty relics. Inside, Finn discovered a faded journal belonging to the pirate captain, Blackbeard. The journal mentioned two other hidden clues needed to find the treasure: a golden compass and a secret code. Finn\u2019s next challenge was to find the golden compass, which was rumored to be hidden in the Whispering Caves. The caves were filled with echoes and",
    "chunk_id": "Small-Stories-in-English-learnenglishteam.com__0"
  },
  {
    "doc": "Small-Stories-in-English-learnenglishteam.com_.pdf",
    "chunk": "tricky paths that seemed to shift and change. Finn used his wits and the map to navigate through the caves, avoiding traps and deciphering clues etched into the walls. At the heart of the Whispering Caves, Finn found the golden compass embedded in a stone pedestal. The compass had intricate carvings and glowed faintly. With the compass in hand, Finn felt a surge of excitement. He now needed to crack the secret code mentioned in Blackbeard\u2019s journal. Returning to the pirate fort, Finn studied the journal carefully. The secret code was hidden in a series of symbols that needed to be aligned in a specific order. Finn used the golden compass to guide him, and after several attempts, he managed to decipher the code. The final clue led Finn to a hidden chamber beneath the fort. The chamber was adorned with old treasure chests, but the true treasure was hidden behind a sliding wall. Finn used the code to unlock the wall, revealing a dazzling array of gold coins, jewels, and precious artifacts. At the center of the treasure was a beautifully crafted chest with a plaque that read: \u201cTo the bravest of hearts.\u201d As Finn marveled at the treasure, Captain Squawk appeared again, fluttering with joy. \u201cYe\u2019ve done it, young adventurer! Ye\u2019ve found the lost treasure of Pi-Rate Island! But remember, true treasure is not just gold and jewels\u2014it\u2019s the adventure and courage ye showed.\u201d Finn agreed, realizing that the journey itself had been as valuable as the treasure. He returned to Seaside with stories of his adventure and the lost treasure. The villagers celebrated his bravery, and Pi-Rate Island became a symbol of adventure and discovery. **Main Idea:** The true treasure lies not just in gold and jewels, but in the courage and adventure experienced along the journey. ----- ## **The Enchanted Library** In the charming village of Willowbrook, there was once a grand library known as the Enchanted Library, famous for its magical books and timeless tales. Sadly, a dark spell had sealed the library for years, leaving its wonders dormant and forgotten. Emily, a bright and adventurous girl with a love for stories, had always been captivated by the tales of the Enchanted Library. Her grandmother often spoke of its wonders, and Emily dreamed of one day exploring its mystical halls. One sunny afternoon, while rummaging through her grandmother\u2019s attic, Emily stumbled upon a hidden compartment in an old wooden chest. Inside, she found an intricately designed key with shimmering runes and a tag that read: \u201cTo awaken the magic, unlock the heart.\u201d Excited, Emily hurried to the library. The once-majestic building was overgrown with ivy and vines. The grand wooden doors were locked tight, but Emily\u2019s key fit perfectly. As she turned it, the doors creaked open with a magical shimmer, revealing a world of enchantment. Inside, the library was bathed in a golden glow. Dust motes danced in the air, and the shelves, though covered in cobwebs, seemed to whisper secrets. Emily approached the grand hall, where a towering book, bound in gold and silver, rested",
    "chunk_id": "Small-Stories-in-English-learnenglishteam.com__1"
  },
  {
    "doc": "Small-Stories-in-English-learnenglishteam.com_.pdf",
    "chunk": "on a pedestal. The book was locked with a complex mechanism. Suddenly, a soft, rustling sound caught Emily\u2019s attention. Out of the shadows emerged a glowing, ancient book named Barnaby. He floated gracefully, his cover adorned with ornate patterns and twinkling stars. \u201cWelcome, Emily,\u201d Barnaby said in a warm, echoing voice. \u201cI\u2019ve been waiting for someone with a pure heart to restore the magic. I am Barnaby, the guardian of this library.\u201d Emily\u2019s eyes widened. \u201cYou know my name?\u201d Barnaby chuckled softly. \u201cI know all who seek the magic of stories. To restore the library, we must unlock the heart of its enchantment. But first, we need to navigate the challenges that have kept it hidden for so long.\u201d Barnaby led Emily through the labyrinthine library, filled with hidden passages and secret rooms. They ventured into the Grand Gallery, a room with walls that were covered in moving paintings and glowing constellations. Each painting depicted scenes from famous stories, but they seemed frozen in time. Emily\u2019s first challenge was to solve a puzzle to unlock a hidden door. The door was adorned with a riddle: \u201cI speak without a mouth and hear without ears. I have no body, but I come alive with wind. What am I?\u201d Barnaby hovered beside her, his pages fluttering with anticipation. Emily thought deeply and answered, \u201cAn echo!\u201d The door creaked open, revealing a hidden chamber filled with old, dusty scrolls. -----",
    "chunk_id": "Small-Stories-in-English-learnenglishteam.com__2"
  },
  {
    "doc": "Small-Stories-in-English-learnenglishteam.com_.pdf",
    "chunk": "In this chamber, Emily and Barnaby discovered a journal with clues to the library\u2019s secrets. The journal spoke of three enchanted relics that held the key to awakening the magic: a mystical feather, a golden quill, and an ancient scroll. Their journey led them to the Hall of Relics, where the feather was hidden. The room was filled with enchanted traps and magical barriers. Emily had to navigate through a series of moving platforms and shifting walls. With Barnaby\u2019s guidance and encouragement, Emily\u2019s quick thinking and bravery helped her retrieve the feather. Next, they ventured to the Whispering Woods, a part of the library filled with living trees and magical creatures. The golden quill was hidden in a tree that spoke in riddles. Emily had to answer the tree\u2019s questions correctly to retrieve the quill. Their final quest took them to the Echoing Cavern, where the ancient scroll was hidden. The cavern was filled with echoes that created illusions and distractions. Emily had to use her wits and Barnaby\u2019s wisdom to find the scroll amidst the confusion. As they gathered the relics, Emily and Barnaby returned to the Grand Hall. The heart of the library was a massive, ancient tree with branches covered in shimmering leaves. Each leaf represented a story waiting to be told. The tree\u2019s base held a grand chest locked with a combination that required solving a series of riddles. Emily and Barnaby worked together to solve the riddles: 1. \u201cI am taken from a mine, and shut up in a wooden case, from which I am never released, and yet I am used by almost every person. What am I?\u201d 2. \u201cI can be cracked, made, told, and played. What am I?\u201d 3. \u201cI have keys but open no locks. I have space but no room. You can enter but not go outside. What am I?\u201d With each correct answer, the chest\u2019s lock clicked open. When Emily finally opened the chest, a brilliant light burst forth, filling the library with warmth and magic. The once-sleeping books awoke, their pages rustling with excitement, and the enchantment of the library was restored. Emily and Barnaby watched as the library transformed into a vibrant, magical place. The books floated gently from the shelves, and the air was filled with the soft whispers of stories coming to life. Emily was given the special gift of bringing stories to life simply by reading them aloud. Returning to Willowbrook, Emily shared her adventure with the villagers, who marveled at the revived Enchanted Library. The library\u2019s doors were now open to everyone, and the village celebrated the magic of stories and the wonders of imagination. Emily\u2019s adventure taught her that the true magic of the library was not just in its books but in the belief and curiosity that could awaken even the deepest enchantments. **Main Idea:** The true magic of stories is unlocked through curiosity, courage, and a belief in the power of imagination. ----- ## **The Brave Little Squirrel** In the heart of the forest, there was a brave little squirrel named Nutty.",
    "chunk_id": "Small-Stories-in-English-learnenglishteam.com__3"
  },
  {
    "doc": "Small-Stories-in-English-learnenglishteam.com_.pdf",
    "chunk": "Nutty was known for his boundless energy and adventurous spirit. One day, a great storm hit the forest, causing chaos and destruction. Many animals were frightened and sought shelter. Nutty, despite being small, decided to help. He gathered his friends and led them to a sturdy, hidden burrow he had discovered. Along the way, he faced challenges: fallen branches, rushing waters, and frightened animals. Nutty\u2019s courage shone through. He helped guide his friends over obstacles, encouraged the fearful animals, and found creative solutions to each problem. He used his quick thinking to navigate the forest and ensure everyone reached safety. When the storm finally passed, the forest animals emerged from the burrow to find their homes damaged. Nutty rallied the animals, and together, they worked to rebuild their homes and restore their forest. Nutty\u2019s bravery and leadership were celebrated, and the forest thrived once more. The animals learned that even the smallest among them could make a big difference. **Main Idea:** Courage and leadership can inspire others and help overcome challenges, even in the toughest times. ## **The Magical Music Box** In a quaint village, there was an old music box that had been in the family of a young girl named Lily for generations. The music box was beautiful, but its melody had long since faded, and it was said to be enchanted. One evening, Lily\u2019s grandmother told her the story of the music box. \u201cLegend has it that the music box holds a magical melody that can grant a wish,\u201d she said. \u201cBut only if it\u2019s played with true love and care.\u201d Determined to uncover the magic, Lily carefully polished the music box and played it with all her heart. To her surprise, the music box began to glow, and a gentle melody filled the air. As the melody played, Lily noticed a small, shimmering fairy appear. \u201cThank you for awakening the music box\u2019s magic,\u201d the fairy said. \u201cI\u2019ve been waiting for someone with a pure heart to restore its magic.\u201d The fairy granted Lily a wish, but instead of wishing for something for herself, Lily wished for happiness and prosperity for her village. The fairy granted the wish, and the village flourished with joy and abundance. ----- Lily learned that true magic comes from selflessness and the desire to help others. **Main Idea:** True magic comes from selflessness and the desire to bring happiness to others. ## **The Little Hero\u2019s Journey** In a bustling city, there was a young boy named Alex who dreamed of becoming a hero. One day, while exploring an old attic, he found a dusty, ancient book titled \u201cThe Hero\u2019s Journey.\u201d As Alex read the book, he was magically transported to a mythical land where a great dragon was terrorizing the kingdom. The kingdom\u2019s only hope was a hero who could retrieve a lost gemstone that could tame the dragon. With courage and determination, Alex set off on his quest. He faced many trials: navigating through enchanted forests, outsmarting tricky trolls, and solving ancient riddles. Each challenge tested his bravery and wisdom. Finally, Alex reached",
    "chunk_id": "Small-Stories-in-English-learnenglishteam.com__4"
  },
  {
    "doc": "Small-Stories-in-English-learnenglishteam.com_.pdf",
    "chunk": "the dragon\u2019s lair and discovered that the dragon was not evil but was suffering from a curse that had made it act out. Using the gemstone, Alex broke the curse and restored peace to the kingdom. The grateful king crowned Alex a hero, and Alex returned to his world with a newfound confidence and the knowledge that bravery comes from within. **Main Idea:** True heroism comes from facing challenges with courage and a willingness to help others. ## **The Secret Garden** In a charming village, there was a neglected garden behind a tall, ivy-covered wall. The garden was once beautiful but had fallen into disrepair. A young girl named Mia, with a love for nature, decided to restore the garden. Mia spent every day working on the garden. She pruned the overgrown plants, cleared the weeds, and planted new flowers. As she worked, she discovered old garden tools and a mysterious, locked gate. One day, an elderly gardener named Mr. Green visited Mia. \u201cI see you\u2019re working hard to restore the garden,\u201d he said. \u201cThe gate you found is a magical one. It can only be unlocked with a special key that has been lost for many years.\u201d ----- Mia was determined to find the key. She searched the village, asking for stories and clues about the garden\u2019s past. Finally, she discovered an old diary with a description of the key\u2019s hiding place. With the key in hand, Mia unlocked the gate and found a hidden part of the garden filled with rare and beautiful plants. She restored it to its former glory, and the village celebrated the renewed beauty of the garden. **Main Idea:** Persistence and dedication can lead to discovering hidden beauty and making a lasting impact. ## **The Mysterious Lighthouse** On a rugged coastline, there stood an old lighthouse that had been abandoned for years. A young boy named Jack, who loved exploring, was fascinated by the lighthouse. One foggy evening, he decided to investigate. As Jack climbed the lighthouse stairs, he noticed strange lights flickering in the distance. He reached the top and discovered that the lighthouse\u2019s light was still operational but not being used. Jack met an old keeper\u2019s ghost named Captain Bill. \u201cI\u2019ve been waiting for someone to help me,\u201d Captain Bill said. \u201cThe lighthouse was cursed, and its light has been lost. Only someone with a pure heart can lift the curse.\u201d Jack agreed to help. He followed Captain Bill\u2019s instructions to solve a series of riddles and repair the lighthouse\u2019s mechanisms. Along the way, Jack faced various challenges: navigating through dark rooms, fixing broken parts, and decoding messages. As Jack completed each task, the lighthouse\u2019s light grew brighter. Finally, the curse was lifted, and the lighthouse shone brightly once more, guiding ships safely to shore. Jack returned to his village, where the lighthouse became a beacon of hope and safety. He learned that courage and a pure heart could overcome even the darkest of curses. **Main Idea:** Courage and a pure heart can lift curses and bring light to even the darkest places. -----",
    "chunk_id": "Small-Stories-in-English-learnenglishteam.com__5"
  },
  {
    "doc": "Small-Stories-in-English-learnenglishteam.com_.pdf",
    "chunk": "## **The Hidden Village** Deep in the heart of an ancient forest, shrouded in mist and mystery, lay the hidden village of Whispering Pines. This magical village was known only to those who truly needed it and appeared only when the forest was in great need of help. One crisp autumn afternoon, a weary traveler named Ella wandered into the forest. She had been lost for days, her spirit dimming with each step. Just as she was about to give up hope, the forest seemed to shift and reveal a hidden path, leading her to the village. As Ella stepped into Whispering Pines, she was greeted by an awe-inspiring sight. The village was unlike anything she had ever seen: floating lanterns bobbed gently in the air, illuminating cobblestone streets lined with quaint cottages covered in vibrant flowers. The villagers\u2014an eclectic mix of magical creatures\u2014welcomed her warmly. The first villager Ella met was a jovial elf named Lorian. His pointy ears twitched with excitement as he said, \u201cWelcome to Whispering Pines! It\u2019s rare to see a newcomer. We\u2019ve been in need of help for a long time.\u201d Ella noticed that the once-bustling village was now quiet and subdued. The lanterns that usually floated with a soft, warm glow were dim, and the animals that once played in the streets seemed restless. As Lorian explained, the village\u2019s magic was fading because they had lost a precious crystal that sustained their enchantment. The crystal, called the Heartstone, had been stolen by a greedy sorcerer who sought to harness its power for himself. Without it, the village\u2019s magic was waning, and its connection to the outside world was slowly disappearing. Determined to help, Ella set out with Lorian and his friends. The first stop was the Enchanted Forest, a mystical part of the forest filled with ancient trees and hidden paths. Ella was introduced to the village\u2019s wise old wizard, Alaric, who had once been a guardian of the Heartstone but had lost his memory and powers after the crystal was stolen. Alaric, though frail and forgetful, gave Ella a magical compass that would guide her to the sorcerer\u2019s lair. He also shared a clue: \u201cThe Heartstone lies where shadows dance and the moonlight sings.\u201d Ella and Lorian ventured deeper into the forest, encountering a series of magical challenges. They navigated through the Whispering Woods, where the trees whispered secrets and misdirected travelers. They befriended a mischievous sprite named Flicker who offered to guide them through the maze of thorns in exchange for a promise of friendship. Flicker led them to the Moonlit Glade, where shadows seemed to come alive. The sorcerer\u2019s lair was hidden in a cave behind a waterfall. As they approached, Ella noticed that the cave was guarded by enchanted creatures\u2014a giant, shimmering serpent with scales like mirrors. Ella used her wits to distract the serpent by creating illusions with her lantern and using the magical compass to find a hidden path around it. Inside the cave, she found the Heartstone atop a pedestal, glowing faintly in the dim light. ----- As Ella",
    "chunk_id": "Small-Stories-in-English-learnenglishteam.com__6"
  },
  {
    "doc": "Small-Stories-in-English-learnenglishteam.com_.pdf",
    "chunk": "reached for the Heartstone, the sorcerer appeared, his eyes gleaming with malice. \u201cYou think you can take it from me?\u201d he sneered. Ella, undeterred, bravely stood her ground. \u201cThe magic of Whispering Pines is meant to be shared, not hoarded for selfish gain.\u201d With a burst of courage, Ella used her lantern to reflect the Heartstone\u2019s light, temporarily blinding the sorcerer. She seized the opportunity to grab the crystal and raced back to the village, Lorian and Flicker by her side. Upon returning, Ella handed the Heartstone to Alaric. As he held it, the crystal\u2019s magic surged back into the village. The lanterns lit up with a vibrant glow, the flowers bloomed with renewed vigor, and the animals danced joyfully in the streets. The villagers celebrated Ella\u2019s bravery with a grand feast. Alaric, now fully restored to his former self, blessed Ella with a gift: a magical locket that would guide her home if she ever needed help again. As Ella left Whispering Pines, she felt a sense of fulfillment and joy. She had not only restored the village\u2019s magic but had also made lifelong friends. The hidden village, once again vibrant and alive, continued to flourish, its magic intertwined with the kindness and bravery of those who came to its aid. **Main Idea:** True magic and wonder are restored through bravery, selflessness, and the connections we make with others. ## **The Brave Little Kitten** Once upon a time in a cozy village, there was a tiny kitten named Whiskers who lived with her grandmother, Nana Tilly. Whiskers was known for her soft fur and gentle purrs, but she was also very curious about the world beyond her village. One sunny morning, Whiskers heard a commotion outside. A colorful carnival had set up just beyond the village boundaries. Excited, Whiskers decided to explore. She ventured out, her little heart pounding with excitement and a touch of fear. As Whiskers approached the carnival, she was amazed by the sights and sounds. There were clowns juggling, colorful tents, and a magical fortune teller. But as Whiskers wandered farther, she noticed that the magical fortune teller\u2019s tent was in trouble. The fortune teller\u2019s crystal ball had fallen and shattered, and she needed help. \u201cCan anyone help?\u201d the fortune teller cried, her voice tinged with worry. Whiskers, despite her nervousness, stepped forward. \u201cI can help! I may be small, but I\u2019m brave.\u201d ----- The fortune teller smiled gratefully. \u201cIf you can find the missing pieces of the crystal ball, I can restore its magic.\u201d With determination, Whiskers scoured the carnival grounds. She encountered various challenges: a mischievous monkey who wouldn\u2019t give up a piece of the ball, a maze of tents, and a huge, playful dog that wanted to play fetch with the remaining pieces. Whiskers faced each challenge with bravery and cleverness. She used her agility to dodge the monkey, her keen senses to navigate the maze, and her charm to convince the dog to return the last piece. When she finally gathered all the pieces, the fortune teller was overjoyed. She carefully reassembled the",
    "chunk_id": "Small-Stories-in-English-learnenglishteam.com__7"
  },
  {
    "doc": "Small-Stories-in-English-learnenglishteam.com_.pdf",
    "chunk": "crystal ball and performed a spell. The crystal ball shimmered with renewed magic, and the carnival came alive with even more wonder and joy. Whiskers returned to her village as a hero, and the carnival became a beloved part of the village\u2019s festivities. She had learned that even the smallest of creatures could make a big difference with courage and determination. **Main Idea:** Bravery and determination can help overcome obstacles and make a significant impact, regardless of size or experience. ## **The Honest Woodcutter** Once, in a peaceful village nestled by a river, lived a humble woodcutter named Tariq. Tariq was known for his honesty and integrity. He worked hard every day, cutting wood from the forest and selling it to support his family. Despite his modest means, he was always respected by his neighbors for his truthful nature. One day, as Tariq was chopping wood near the riverbank, he accidentally dropped his axe into the deep, murky water. Despair washed over him, as the axe was his only tool for earning a living. Tariq sat by the river, feeling lost, when a spirit appeared from the water. The spirit, known for its wisdom and fairness, greeted him with a kind smile. \u201cI am the guardian of this river,\u201d the spirit said. \u201cI have heard of your honesty and wish to help you. I will retrieve your axe, but first, you must tell me the truth. Is it a silver axe, a golden axe, or the one you lost?\u201d Tariq thought for a moment. Although he had only lost a simple iron axe, he could see that the spirit had a golden and silver axe as well. It would have been easy to claim one of these more valuable axes as his own, but Tariq knew that honesty was more valuable than any material possession. \u201cSir,\u201d Tariq said, \u201cI lost an iron axe. I see that you have golden and silver axes, but none of these are mine.\u201d The spirit nodded, impressed by Tariq\u2019s honesty. With a graceful motion, the spirit reached into the river and brought forth Tariq\u2019s simple iron axe. Tariq\u2019s face lit up with joy and relief. ----- \u201cYou have shown great honesty today,\u201d the spirit said. \u201cAs a reward, I will grant you a special gift.\u201d The spirit then revealed the golden and silver axes. \u201cThese are yours now, not because you asked for them, but because your honesty and integrity have been recognized.\u201d Tariq was astonished and grateful. He took the golden and silver axes, but he did not let them change his humble demeanor. He continued his work with the same dedication and fairness, using the new tools to help others in the village. His honesty had not only preserved his livelihood but also earned him a place of honor in the hearts of those around him. In time, Tariq\u2019s story spread far and wide. He became a symbol of integrity and truthfulness. The villagers admired him not just for his success but for his unwavering commitment to honesty. **Main Idea:** Honesty and integrity bring true",
    "chunk_id": "Small-Stories-in-English-learnenglishteam.com__8"
  },
  {
    "doc": "Small-Stories-in-English-learnenglishteam.com_.pdf",
    "chunk": "rewards, fostering trust and respect in relationships and within the community. -----",
    "chunk_id": "Small-Stories-in-English-learnenglishteam.com__9"
  }
]